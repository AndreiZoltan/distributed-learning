{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1069582",
   "metadata": {},
   "source": [
    "# Model Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829c6ba",
   "metadata": {},
   "source": [
    "## Tensor Parallelism\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "![img](https://miro.medium.com/max/1400/1*qyc21qM0oxWEuRaj-XJKcw.png)\n",
    "\n",
    "[paper](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "Попробуем реализовать вычисления Conv на разных картах!\n",
    "\n",
    "### Megatron LM\n",
    "\n",
    "![img](https://opengraph.githubassets.com/8f1b486eb3b73ba241c8c03783f0e465f7712eacd8125e7ca03b443cb4a5b40e/NVIDIA/Megatron-LM)\n",
    "\n",
    "[arxiv](https://arxiv.org/abs/1909.08053)\n",
    "\n",
    "Реализуем MLP часть!\n",
    "\n",
    "### Alpa (operation parallelism)\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/alpa-projects/alpa/blob/main/docs/logo/alpa-logo-cropped.png?raw=true\" width=\"40%\">  \n",
    "</div>\n",
    "\n",
    "[github](https://github.com/alpa-projects/alpa)\n",
    "\n",
    "[slides](https://docs.google.com/presentation/d/1CQ4S1ff8yURk9XmL5lpQOoMMlsjw4m0zPS6zYDcyp7Y/edit#slide=id.g136a86a0982_0_0)\n",
    "\n",
    "Посмотрим на слайды!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c89129",
   "metadata": {},
   "source": [
    "## Pipeline Parallelism\n",
    "\n",
    "### Torch Pipeline Parallel\n",
    "\n",
    "![img](https://pytorch.org/docs/stable/_images/no_pipe.png)\n",
    "\n",
    "[docs](https://pytorch.org/docs/stable/pipeline.html)\n",
    "\n",
    "Посмотрим на доку и попробуем частично реализовать.\n",
    "\n",
    "### GPipe\n",
    "\n",
    "![img](https://1.bp.blogspot.com/-fXZxDPKaEaw/XHlt7OEoMtI/AAAAAAAAD0I/hYM_6uq2BTwaunHZRxWd7JUJV43fEysvACLcBGAs/s640/image2.png)\n",
    "[arxiv](https://arxiv.org/abs/1811.06965)\n",
    "\n",
    "Посмотрим на статью!\n",
    "\n",
    "### Megatron LM (Again)\n",
    "\n",
    "![img](https://opengraph.githubassets.com/8f1b486eb3b73ba241c8c03783f0e465f7712eacd8125e7ca03b443cb4a5b40e/NVIDIA/Megatron-LM)\n",
    "\n",
    "[arxiv](https://arxiv.org/abs/2104.04473)\n",
    "\n",
    "Посмотрим на статью!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2edbd",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. Добавить блоки MLP, которые могут считаться на нескольких картах. Если нескольких карт нет -- то считаются на одной. Степень параллелизма задается числом карт. На каждой карте должен находится свой процесс!\n",
    "2. Либо как сделать в torch.Module автоматическую замену последовательности Linear на tensor-parallel MLP\n",
    "3. Либо добавить автоматический Torch Pipeline Parallel в код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da372dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_paralelized(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "--pipe-parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
